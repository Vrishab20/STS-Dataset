# üìö Corpus Analysis on Atticus Legal Contracts Dataset

This project analyzes the **Atticus legal contracts dataset** using Natural Language Toolkit (NLTK). It computes corpus-level insights such as token metrics, filtered tokens, and bigrams using common NLP techniques.

---

## ‚öôÔ∏è Installation & Dependencies

### üîπ 1. Install Dependencies
```bash
pip install -r requirements.txt
```

If using Google Colab:
```python
!pip install -r requirements.txt
```

### üîπ 2. Required Python Libraries
- os  
- nltk  
- word_tokenize  
- stopwords  
- bigrams  
- Counter  

---

## ‚ñ∂Ô∏è How to Run the Code

### üîπ 1. Prepare the Dataset
Download the Atticus legal contracts dataset and:
- Unzip and extract `full_contract_txt` into the `CUAD_v1/` directory.

### üîπ 2. Run the Main Script
```bash
python main.py
```

This script:
- Loads NLTK resources  
- Concatenates all contract files into a corpus  
- Tokenizes and filters the text  
- Saves intermediate results (`output.txt`, `tokens.txt`, `word_tokens.txt`, `filtered_word_tokens.txt`, `bigrams.txt`)  
- Generates a summary of results

---

## üìù Summary of the Results
- Extracted total/unique tokens  
- Calculated type-token ratio  
- Generated filtered tokens and bigrams  
- Saved processed data and summary in respective files  

---

# ‚ú® Sentence Embedding Models for STS2016 Evaluation

This project evaluates various **sentence embedding models** on the STS2016 dataset, computing semantic similarity scores and Pearson correlation with gold-standard values.

---

## ‚öôÔ∏è Installation & Dependencies

### üîπ 1. Install Dependencies
```bash
pip install -r requirements.txt
```

If using Google Colab:
```python
!pip install -r requirements.txt
```

### üîπ 2. Required Python Libraries
- numpy  
- scipy  
- transformers  
- sentence-transformers  
- tensorflow-hub  
- torch  
- scikit-learn  

---

## ‚ñ∂Ô∏è How to Run the Code

### üîπ 1. Prepare the Dataset
Download STS2016 dataset and place the following files into `sts2016-english-with-gs-v1.0/`:
- `STS2016.input.*.txt`  
- `STS2016.gs.*.txt`

### üîπ 2. Run the Main Script
```bash
python main.py
```
This script:
- Loads SBERT, RoBERTa, USE, ALBERT, LLaMA-2  
- Embeds sentence pairs  
- Computes cosine similarity and normalizes scores (0‚Äì5 scale)  
- Outputs results to `*_SYSTEM_OUT.*.txt`

### üîπ 3. Compute Pearson Correlation
```bash
python compute_pearson.py
```
Compares predictions with gold scores and computes Pearson correlation.

---

## üß† Models Used & Parameters

| Model       | Embedding Dim | Pooling | Tokenizer           | Fine-tuned? | Framework | Device |
|-------------|----------------|---------|----------------------|-------------|-----------|--------|
| SBERT       | 384            | Mean    | WordPiece            | No          | PyTorch   | GPU    |
| RoBERTa     | 768            | CLS     | SentencePiece        | No          | PyTorch   | GPU    |
| USE         | 512            | Mean    | TensorFlow Tokenizer | No          | TensorFlow| CPU    |
| ALBERT      | 768            | Mean    | WordPiece            | No          | PyTorch   | GPU    |
| LLaMA-2     | 4096           | Mean    | SentencePiece        | No          | PyTorch   | GPU    |

---

## üìÅ File Structure

```
üìÇ project-directory/
‚îú‚îÄ‚îÄ üìÇ sts2016-english-with-gs-v1.0/   # Dataset folder
‚îú‚îÄ‚îÄ üìù requirements.txt                # Dependencies
‚îú‚îÄ‚îÄ üìù README.md                       # This file
‚îú‚îÄ‚îÄ üìú main.py                         # Main execution script
‚îú‚îÄ‚îÄ üìú compute_pearson.py              # Pearson correlation script
‚îú‚îÄ‚îÄ üìú *_SYSTEM_OUT.*.txt              # Model output files
```

---

## ‚ùó Troubleshooting

### 1Ô∏è‚É£ Running Out of Memory?
- Use GPU runtime in Google Colab: `Runtime > Change runtime type > GPU`  
- Reduce batch size for large files  

### 2Ô∏è‚É£ Hugging Face Model Access Issues?
- LLaMA-2 requires Hugging Face login:  
```bash
huggingface-cli login
```
- Ensure access to `meta-llama/Llama-2-7b-chat-hf`

---
